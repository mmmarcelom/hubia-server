# ğŸš€ HubIA Workflow Server

Servidor Python para processamento distribuÃ­do de tarefas usando Ollama com modelos locais (Gemma2, LLaVA, Nomic-Embed-Text) e PyTorch com suporte CUDA.

## ğŸ“‹ Funcionalidades

- **Worker DistribuÃ­do**: Polling de API na nuvem para receber tarefas
- **TranscriÃ§Ã£o de Ãudio**: Usando Gemma2 9B (processamento de texto de Ã¡udio)
- **DescriÃ§Ã£o de Imagens**: Usando LLaVA 7B  
- **SumarizaÃ§Ã£o de Documentos**: Usando Gemma2 9B
- **GeraÃ§Ã£o de Embeddings**: Usando Nomic-Embed-Text
- **GeraÃ§Ã£o de Respostas**: Usando Gemma2 9B (otimizado para portuguÃªs)
- **Suporte CUDA**: PyTorch, TorchAudio, TorchVision com GPU
- **Container Docker**: Pronto para produÃ§Ã£o

## ğŸ› ï¸ Tecnologias

- **Python 3.11**
- **Ollama** - Modelos de IA locais
- **PyTorch + CUDA** - Processamento com GPU
- **httpx** - Cliente HTTP assÃ­ncrono
- **Docker** - ContainerizaÃ§Ã£o
- **Loguru** - Logging avanÃ§ado

## ğŸš€ InstalaÃ§Ã£o e Uso

### 1. PrÃ©-requisitos

- Docker com suporte NVIDIA GPU
- NVIDIA Driver instalado
- Ollama instalado localmente (opcional)

### 2. ConfiguraÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone <seu-repositorio>
cd hubia-workflow-server

# Copie o arquivo de configuraÃ§Ã£o
cp .env.example .env

# Edite as configuraÃ§Ãµes se necessÃ¡rio
nano .env
```

### 3. Configurar Modelos Ollama

```bash
# Torne o script executÃ¡vel
chmod +x scripts/download.sh

# Execute o script de download
./scripts/download.sh
```

### 4. Iniciar o Servidor

```bash
# Usando Docker Compose (recomendado)
docker-compose up --build

# Ou usando docker-compose diretamente
docker-compose up --build
```

### 5. Verificar Status

```bash
# Ver logs do servidor
docker-compose logs -f hubia-workflow-server

# Verificar status dos containers
docker-compose ps

# Verificar status do processo
docker exec hubia-workflow-server pgrep -f "python src/main.py"
```

## ğŸ“¡ API Endpoints

O servidor implementa o protocolo de workflow descrito na documentaÃ§Ã£o:

### Polling para Trabalho
```http
GET /api/v1/workflow/next
Headers:
  x-server-token: sk_meu_servidor_001
```

### Envio de Resposta
```http
POST /api/v1/workflow/responses
Headers:
  x-server-token: sk_meu_servidor_001
  Content-Type: application/json
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

| VariÃ¡vel | DescriÃ§Ã£o | PadrÃ£o |
|----------|-----------|---------|
| `API_URL` | URL da API de workflow | `https://api.hubia.com` |
| `SERVER_KEY` | Token de autenticaÃ§Ã£o (x-server-token) | `sk_meu_servidor_001` |
| `SERVER_NAME` | Nome do servidor na API | `Servidor Local HubIA` |
| `SLUG` | Nome curto proprietÃ¡rio do servidor | `servidor-local-001` |
| `OLLAMA_BASE_URL` | URL do Ollama | `http://localhost:11434` |
| `OLLAMA_MODEL_TRANSCRICAO` | Modelo para transcriÃ§Ã£o | `whisper` |
| `OLLAMA_MODEL_VISAO` | Modelo para descriÃ§Ã£o de imagens | `llava:7b` |
| `OLLAMA_MODEL_RESUMO` | Modelo para sumarizaÃ§Ã£o | `gemma2:9b` |
| `OLLAMA_MODEL_EMBEDDINGS` | Modelo para embeddings | `nomic-embed-text` |
| `OLLAMA_MODEL_CONVERSACAO` | Modelo para respostas | `gemma2:9b` |
| `MAX_AUDIO_SIZE_MB` | Tamanho mÃ¡ximo de Ã¡udio | `50` |
| `MAX_IMAGE_SIZE_MB` | Tamanho mÃ¡ximo de imagem | `20` |
| `MAX_DOCUMENT_SIZE_MB` | Tamanho mÃ¡ximo de documento | `10` |
| `POLLING_INTERVAL_SECONDS` | Intervalo de polling | `5` |

## ğŸ“š DocumentaÃ§Ã£o

- **[Estrutura do Projeto](docs/PROJECT_STRUCTURE.md)** - OrganizaÃ§Ã£o de pastas e mÃ³dulos
- **[Workflow API](docs/README_WORKFLOW.md)** - Guia completo do workflow
- **[Docker](docs/README_DOCKER.md)** - ExecuÃ§Ã£o com Docker
- **[API Development Guide](docs/WORKFLOW_API_DEVELOPMENT_GUIDE.md)** - Guia de desenvolvimento da API

## ğŸ”§ Desenvolvimento

### Estrutura do Projeto

```
hubia-workflow-server/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Ponto de entrada
â”‚   â”œâ”€â”€ core/                   # MÃ³dulo core
â”‚   â”‚   â”œâ”€â”€ config.py          # ConfiguraÃ§Ãµes
â”‚   â”‚   â””â”€â”€ (removido)         # Modelos de dados nÃ£o necessÃ¡rios
â”‚   â”œâ”€â”€ workflow/              # MÃ³dulo workflow
â”‚   â”‚   â””â”€â”€ workflow_client.py # Cliente de workflow
â”‚   â””â”€â”€ processors/            # Processadores de mÃ­dia
â”‚       â”œâ”€â”€ base_processor.py
â”‚       â”œâ”€â”€ audio_processor.py
â”‚       â”œâ”€â”€ image_processor.py
â”‚       â”œâ”€â”€ document_processor.py
â”‚       â”œâ”€â”€ text_processor.py
â”‚       â””â”€â”€ prompt_processor.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download.sh            # Download de modelos
â”‚   â”œâ”€â”€ start.sh               # Script de inicializaÃ§Ã£o (Docker)
â”‚   â”œâ”€â”€ register.py            # Registro de servidor
â”‚   â””â”€â”€ test_workflow.py       # Teste do workflow
â”œâ”€â”€ docs/                      # DocumentaÃ§Ã£o
â”œâ”€â”€ docker-compose.yml         # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile                 # Imagem Docker
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â””â”€â”€ .env.example              # ConfiguraÃ§Ã£o de exemplo
```

### Executar em Desenvolvimento

```bash
# Instalar PyTorch com CUDA separadamente (melhor prÃ¡tica)
pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu128

# Instalar outras dependÃªncias
pip install -r requirements.txt

# Configurar variÃ¡veis de ambiente
export API_URL=https://api.hubia.com
export SERVER_KEY=sk_meu_servidor_001
export SERVER_NAME="Servidor Local HubIA"
export SLUG=servidor-local-001

# Executar servidor
python src/main.py
```

### Scripts de ConfiguraÃ§Ã£o

```bash
# Configurar modelos Ollama
./scripts/download.sh
```

## ğŸ“Š Monitoramento

### Logs

Os logs sÃ£o salvos em:
- Console: SaÃ­da padrÃ£o
- Arquivo: `/app/logs/workflow_server.log`

### Monitoramento

```bash
# Verificar se o processo estÃ¡ rodando
docker exec hubia-workflow-server pgrep -f "python src/main.py"

# Verificar logs para status
docker-compose logs hubia-workflow-server | tail -20

# Verificar trabalhos processados
docker-compose logs hubia-workflow-server | grep "Trabalho encontrado"
```

### MÃ©tricas

O servidor registra mÃ©tricas de:
- Trabalhos processados
- Tempo de processamento
- Erros e falhas
- Uso de recursos
- Status de conectividade com a API

## ğŸ› Troubleshooting

### Problemas Comuns

1. **Erro de CUDA**: Verifique se o driver NVIDIA estÃ¡ instalado
2. **Ollama nÃ£o responde**: Verifique se o Ollama estÃ¡ rodando na porta 11434
3. **Modelos nÃ£o encontrados**: Execute `./scripts/download.sh`
4. **Erro de permissÃ£o**: Verifique permissÃµes dos diretÃ³rios `logs/`, `temp/`

### Logs de Debug

```bash
# Ver logs detalhados
docker-compose logs -f hubia-workflow-server

# Logs com nÃ­vel DEBUG
export LOG_LEVEL=DEBUG
docker-compose up --build
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ†˜ Suporte

Para suporte e dÃºvidas:
- Abra uma issue no GitHub
- Consulte a documentaÃ§Ã£o da API
- Verifique os logs do servidor
